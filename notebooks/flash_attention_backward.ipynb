{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bbecb-cf15-4a38-b5c2-74ee9777e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x, dim=-1).values[:, None])\n",
    "    return e_x / torch.sum(e_x, dim=-1)[:, None]\n",
    "\n",
    "def compute_attention(Q, K, V):\n",
    "    attention_scores = Q @ K.T\n",
    "    attention_weights = softmax((attention_scores) * tau)\n",
    "    O = torch.matmul(attention_weights, V)\n",
    "    return O\n",
    "\n",
    "def flash_attention(Q, K, V): \n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(N, dtype=torch.float64)\n",
    "    m = -np.inf * torch.ones(N, dtype=torch.float64)\n",
    "    T_r = int(np.ceil(N / B_r))\n",
    "    T_c = int(np.ceil(N / B_c))\n",
    "    for j in range(T_c):\n",
    "        K_j = K[j * B_c:(j + 1) * B_c]\n",
    "        V_j = V[j * B_c:(j + 1) * B_c]\n",
    "        for i in range(T_r):\n",
    "            Q_i = Q[i * B_r:(i + 1) * B_r]\n",
    "            O_i = O[i * B_r:(i + 1) * B_r]\n",
    "            l_i = l[i * B_r:(i + 1) * B_r]\n",
    "            m_i = m[i * B_r:(i + 1) * B_r]\n",
    "    \n",
    "            S_ij = tau * (Q_i @ K_j.T) \n",
    "            m_ij = torch.max(S_ij, dim=1).values\n",
    "            P_ij = torch.exp(S_ij - m_ij[:, None]) \n",
    "\n",
    "            l_ij = torch.sum(P_ij, dim=1)\n",
    "            m_new = torch.maximum(m_i, m_ij) \n",
    "\n",
    "            l_new = torch.exp(m_i - m_new) * l_i + torch.exp(m_ij - m_new) * l_ij\n",
    "\n",
    "            O[i * B_r:(i + 1) * B_r] = (1.0/l_new)[:, None] * ((torch.exp(m_i - m_new) * l_i)[:, None] * O_i +  (torch.exp(m_ij - m_new)[:, None] * (P_ij @ V_j)))\n",
    "            m[i * B_r:(i + 1) * B_r] = m_new\n",
    "            \n",
    "            l[i * B_r:(i + 1) * B_r] = l_new\n",
    "    \n",
    "    return O, l, m\n",
    "\n",
    "def flash_attention_backward(Q, K, V, O_flash, dO): \n",
    "    \n",
    "    dQ = torch.zeros_like(Q)\n",
    "    dK = torch.zeros_like(K)\n",
    "    dV = torch.zeros_like(V)\n",
    "    T_r = int(np.ceil(N / B_r))\n",
    "    T_c = int(np.ceil(N / B_c))\n",
    "    for j in range(T_c):\n",
    "        K_j = K[j * B_c:(j + 1) * B_c]\n",
    "        V_j = V[j * B_c:(j + 1) * B_c]\n",
    "        dK_j = dK[j * B_c:(j + 1) * B_c]\n",
    "        dV_j = dV[j * B_c:(j + 1) * B_c]\n",
    "        for i in range(T_r):\n",
    "            Q_i = Q[i * B_r:(i + 1) * B_r]\n",
    "            O_i = O_flash[i * B_r:(i + 1) * B_r]\n",
    "            dQ_i = dQ[i * B_r:(i + 1) * B_r]\n",
    "            dO_i = dO[i * B_r:(i + 1) * B_r]\n",
    "            l_i = l[i * B_r:(i + 1) * B_r]\n",
    "            m_i = m[i * B_r:(i + 1) * B_r]\n",
    "            \n",
    "            S_ij = tau * (Q_i @ K_j.T) \n",
    "            P_ij = (1.0/l_i)[:, None] * torch.exp(S_ij - m_i[:, None])  \n",
    "    \n",
    "            dV_j = dV_j + (P_ij.T @ dO_i)\n",
    "            dP_ij = dO_i @ V_j.T\n",
    "    \n",
    "            D_i = (dO_i * O_i).sum(dim=1)\n",
    "    \n",
    "            dS_ij = P_ij * (dP_ij - D_i[:, None])\n",
    "            #dQ_i = dQ_i + (tau * dS_ij @ K_j)\n",
    "    \n",
    "            dQ[i * B_r:(i + 1) * B_r] = dQ_i + (tau * dS_ij @ K_j)\n",
    "            dK_j = dK_j + tau * dS_ij.T @ Q_i\n",
    "            \n",
    "        dK[j * B_c:(j + 1) * B_c] = dK_j\n",
    "        dV[j * B_c:(j + 1) * B_c] = dV_j\n",
    "    return dQ, dK, dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5625e-b34a-4a6a-bcdd-8ec4010647d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 334\n",
    "d = 233\n",
    "M = d * 25\n",
    "tau = 1.0/np.sqrt(d)\n",
    "\n",
    "np.random.seed(2)\n",
    "Q = np.random.randn(N, d)\n",
    "K = np.random.randn(N, d)\n",
    "V = np.random.randn(N, d)\n",
    "\n",
    "Q_standard = torch.tensor(Q.copy(), requires_grad=True, dtype=torch.float64)\n",
    "K_standard = torch.tensor(K.copy(), requires_grad=True, dtype=torch.float64)\n",
    "V_standard = torch.tensor(V.copy(), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "O_standard = compute_attention(Q_standard, K_standard, V_standard)\n",
    "loss = O_standard.sum()\n",
    "loss.backward()\n",
    "\n",
    "Q = torch.tensor(Q, dtype=torch.float64)\n",
    "K = torch.tensor(K, dtype=torch.float64)\n",
    "V = torch.tensor(V, dtype=torch.float64)\n",
    "\n",
    "on_chip_memory_size = M\n",
    "B_c = on_chip_memory_size // (4 * d)  # Using 4 bytes per float\n",
    "B_r = min(on_chip_memory_size // (4 * d), d)\n",
    "\n",
    "O_flash, l, m = flash_attention(Q, K, V)\n",
    "dO = torch.ones_like(O_flash)\n",
    "dQ, dK, dV = flash_attention_backward(Q, K, V, O_flash, dO)\n",
    "\n",
    "print(\"Forward O\", np.allclose(O_standard.detach().cpu().numpy(), O_flash.detach().cpu().numpy(), rtol=1.e-4, atol=1.e-4))\n",
    "print(\"Backward V\",np.allclose(V_standard.grad.detach().cpu().numpy(), dV.detach().cpu().numpy(), rtol=1.e-4, atol=1.e-4))\n",
    "print(\"Backward K\",np.allclose(K_standard.grad.detach().cpu().numpy(), dK.detach().cpu().numpy(), rtol=1.e-4, atol=1.e-4))\n",
    "print(\"Backward Q\",np.allclose(Q_standard.grad.detach().cpu().numpy(), dQ.detach().cpu().numpy(), rtol=1.e-4, atol=1.e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca539b65-cd56-4a5b-81ad-1978f451f719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
