{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30e3eef-b0d0-466a-9507-83ea15e5fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x, dim=-1).values[:, None])\n",
    "    return e_x / torch.sum(e_x, dim=-1)[:, None]\n",
    "\n",
    "def compute_attention(Q, K, V):\n",
    "    attention_scores = Q @ K.T\n",
    "    attention_weights = softmax((attention_scores) * tau)\n",
    "    O = torch.matmul(attention_weights, V)\n",
    "    return O\n",
    "\n",
    "def flash_attention(Q, K, V):\n",
    "    \n",
    "    O = torch.zeros((N, d)).float()\n",
    "    L = torch.zeros((N, )).float()\n",
    "    m = -np.inf * torch.ones(N).float()\n",
    "    l = torch.zeros(N).float()\n",
    "    \n",
    "    T_r = int(np.ceil(N / B_r))\n",
    "    T_c = int(np.ceil(N / B_c))\n",
    "    \n",
    "    for i in range(T_r):\n",
    "        Q_i = Q[i * B_r:(i + 1) * B_r]\n",
    "        \n",
    "        m_ij_prev = m[i * B_r:(i + 1) * B_r]\n",
    "        l_ij_prev = l[i * B_r:(i + 1) * B_r]\n",
    "        O_ij_prev = O[i * B_r:(i + 1) * B_r]\n",
    "        \n",
    "    \n",
    "        for j in range(T_c):\n",
    "                \n",
    "            K_j = K[j * B_c:(j + 1) * B_c]\n",
    "            V_j = V[j * B_c:(j + 1) * B_c]\n",
    "        \n",
    "            S_ij = tau * (Q_i @ K_j.T) \n",
    "\n",
    "            m_ij = torch.maximum(m_ij_prev, torch.max(S_ij, dim=1).values)\n",
    "            P_ij = torch.exp(S_ij - m_ij[:, None]) \n",
    "            l_ij = torch.exp(m_ij_prev - m_ij) * l_ij_prev + torch.sum(P_ij, dim=1)\n",
    "                \n",
    "            O_ij = torch.exp(m_ij_prev - m_ij)[:, None] * O_ij_prev + (P_ij @ V_j)\n",
    "            \n",
    "            O_ij_prev = O_ij\n",
    "            m_ij_prev = m_ij\n",
    "            l_ij_prev = l_ij\n",
    "            \n",
    "        O[i * B_r:(i + 1) * B_r] = (1 / l_ij_prev)[:, None] * O_ij_prev \n",
    "        L[i * B_r:(i + 1) * B_r] = m_ij_prev + torch.log(l_ij_prev)\n",
    "        \n",
    "    return O, L\n",
    "\n",
    "def flash_attention_backward(Q, K, V, O_flash, dO): \n",
    "    \n",
    "    dO = dO.double()\n",
    "    dQ = torch.zeros_like(Q).double()\n",
    "    dK = torch.zeros_like(K).double()\n",
    "    dV = torch.zeros_like(V).double()\n",
    "    T_r = int(np.ceil(N / B_r))\n",
    "    T_c = int(np.ceil(N / B_c))\n",
    "    \n",
    "    D = torch.sum(dO * O_flash, dim=1)\n",
    "    \n",
    "    for j in range(T_c):\n",
    "        K_j = K[j * B_c:(j + 1) * B_c]\n",
    "        V_j = V[j * B_c:(j + 1) * B_c]\n",
    "        dK_j = dK[j * B_c:(j + 1) * B_c]\n",
    "        dV_j = dV[j * B_c:(j + 1) * B_c]\n",
    "        for i in range(T_r):\n",
    "            Q_i = Q[i * B_r:(i + 1) * B_r]\n",
    "            O_i = O_flash[i * B_r:(i + 1) * B_r]\n",
    "            dQ_i = dQ[i * B_r:(i + 1) * B_r]\n",
    "            dO_i = dO[i * B_r:(i + 1) * B_r]\n",
    "            L_i = L[i * B_r:(i + 1) * B_r]\n",
    "            D_i = D[i * B_r:(i + 1) * B_r]\n",
    "            # m_i = m[i * B_r:(i + 1) * B_r]\n",
    "            \n",
    "            S_ij = tau * (Q_i @ K_j.T) \n",
    "            # P_ij = (1.0/l_i)[:, None] * torch.exp(S_ij - m_i[:, None])  \n",
    "            P_ij = torch.exp(S_ij - L_i[:, None])  \n",
    "            \n",
    "\n",
    "    \n",
    "            dV_j = dV_j + (P_ij.T @ dO_i)\n",
    "            dP_ij = dO_i @ V_j.T\n",
    "    \n",
    "            dS_ij = P_ij * (dP_ij - D_i[:, None])\n",
    "            #dQ_i = dQ_i + (tau * dS_ij @ K_j)\n",
    "    \n",
    "            dQ[i * B_r:(i + 1) * B_r] = dQ_i + (tau * dS_ij @ K_j)\n",
    "            dK_j = dK_j + tau * dS_ij.T @ Q_i\n",
    "            \n",
    "        dK[j * B_c:(j + 1) * B_c] = dK_j\n",
    "        dV[j * B_c:(j + 1) * B_c] = dV_j\n",
    "    return dQ, dK, dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24523123-0d86-4941-902e-0780ab47c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward O True\n",
      "Backward V True\n",
      "Backward K True\n",
      "Backward Q True\n"
     ]
    }
   ],
   "source": [
    "N = 334\n",
    "d = 233\n",
    "M = d * 25\n",
    "tau = 1.0/np.sqrt(d)\n",
    "\n",
    "np.random.seed(2)\n",
    "Q = np.random.randn(N, d)\n",
    "K = np.random.randn(N, d)\n",
    "V = np.random.randn(N, d)\n",
    "\n",
    "Q_standard = torch.tensor(Q.copy(), requires_grad=True, dtype=torch.float64)\n",
    "K_standard = torch.tensor(K.copy(), requires_grad=True, dtype=torch.float64)\n",
    "V_standard = torch.tensor(V.copy(), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "O_standard = compute_attention(Q_standard, K_standard, V_standard)\n",
    "loss = O_standard.sum()\n",
    "loss.backward()\n",
    "\n",
    "Q = torch.tensor(Q, dtype=torch.float64)\n",
    "K = torch.tensor(K, dtype=torch.float64)\n",
    "V = torch.tensor(V, dtype=torch.float64)\n",
    "\n",
    "on_chip_memory_size = M\n",
    "B_c = on_chip_memory_size // (4 * d)  # Using 4 bytes per float\n",
    "B_r = min(on_chip_memory_size // (4 * d), d)\n",
    "\n",
    "\n",
    "O_flash, L = flash_attention(Q, K, V)\n",
    "dO = torch.ones_like(O_flash)\n",
    "dQ, dK, dV = flash_attention_backward(Q, K, V, O_flash, dO)\n",
    "\n",
    "\n",
    "print(\"Forward O\", np.allclose(O_standard.detach().cpu().numpy(), O_flash.detach().cpu().numpy(), rtol=1.e-4, atol=1.e-4))\n",
    "print(\"Backward V\",np.allclose(V_standard.grad.detach().cpu().numpy(), dV.detach().cpu().numpy(), rtol=1.e-4, atol=1.e-4))\n",
    "print(\"Backward K\",np.allclose(K_standard.grad.detach().cpu().numpy(), dK.detach().cpu().numpy(), rtol=1.e-4, atol=1.e-4))\n",
    "print(\"Backward Q\",np.allclose(Q_standard.grad.detach().cpu().numpy(), dQ.detach().cpu().numpy(), rtol=1.e-4, atol=1.e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddcd6c-1daf-4741-b6d4-73b623ff0b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
